{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on TWD tutorial:\n",
    "https://towardsdatascience.com/web-scraping-craigslist-a-complete-tutorial-c41cea4f4981?gi=78313d2324de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_query = 'https://vancouver.craigslist.org/search/apa?search_distance=2&postal=V5S4C6&availabilityMode=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vancouver, BC apts/housing for rent  - craigslist\n",
      "103 Records on 1 page(s)\n"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import re\n",
    "from random import randint #avoid throttling by not sending too many requests one after the other\n",
    "from warnings import warn\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#get the first page of the east bay housing prices\n",
    "response = get(str_query)\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#get the macro-container for the housing posts\n",
    "posts = html_soup.find_all('li', class_= 'result-row')\n",
    "#print(type(posts), len(posts)) #check that I got a ResultSet & 120 records\n",
    "\n",
    "#find the total number of posts to find the limit of the pagination\n",
    "results_num = html_soup.find('div', class_= 'search-legend')\n",
    "results_total = int(results_num.find('span', class_='totalcount').text) #pulled the total count of posts as the upper bound of the pages array\n",
    "\n",
    "print(html_soup.title.string)\n",
    "print(results_total, \"Records on\", math.ceil(results_total/120), \"page(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 scraped successfully.\n",
      "\n",
      "Scrape complete! Number of records: 97\n"
     ]
    }
   ],
   "source": [
    "#build out the loop\n",
    "\n",
    "#each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. So we need to step in size 120 in the np.arange function\n",
    "pages = np.arange(0, results_total+1, 120)\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "post_timing = []\n",
    "post_hoods = []\n",
    "post_title_texts = []\n",
    "bedroom_counts = []\n",
    "sqfts = []\n",
    "post_links = []\n",
    "post_prices = []\n",
    "\n",
    "collection_latitude = []\n",
    "collection_longitude = []\n",
    "collection_accuracy = []\n",
    "collection_attribute = []\n",
    "post_body = []\n",
    "\n",
    "for page in pages:\n",
    "    \n",
    "    #get request\n",
    "    response = get(str_query            \n",
    "                   + \"s=\" #the parameter for defining the page number \n",
    "                   + str(page) #the page number in the pages array from earlier\n",
    "                  )\n",
    "\n",
    "    sleep(randint(1,5))\n",
    "     \n",
    "    #throw warning for status codes that are not 200\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "        \n",
    "    #define the html text\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #define the posts\n",
    "    posts = html_soup.find_all('li', class_= 'result-row')\n",
    "        \n",
    "    #extract data item-wise\n",
    "    for post in posts:\n",
    "\n",
    "        if post.find('span', class_ = 'result-hood') is not None:\n",
    "\n",
    "            #posting date\n",
    "            #grab the datetime element 0 for date and 1 for time\n",
    "            post_datetime = post.find('time', class_= 'result-date')['datetime']\n",
    "            post_timing.append(post_datetime)\n",
    "\n",
    "            #neighborhoods\n",
    "            post_hood = post.find('span', class_= 'result-hood').text\n",
    "            post_hoods.append(post_hood)\n",
    "\n",
    "            #title text\n",
    "            post_title = post.find('a', class_='result-title hdrlnk')\n",
    "            post_title_text = post_title.text\n",
    "            post_title_texts.append(post_title_text)\n",
    "\n",
    "            #post link\n",
    "            post_link = post_title['href']\n",
    "            post_links.append(post_link)\n",
    "            \n",
    "            #parse post at the above link\n",
    "            post_link_response = get(post_link)\n",
    "            soup = BeautifulSoup(post_link_response.text, 'html.parser')   \n",
    "            \n",
    "            if soup.find('section', id = 'postingbody') is not None:\n",
    "                    post_body.append(soup.find('section', id = 'postingbody').text.strip())\n",
    "            else:\n",
    "                    post_body.append(np.nan)\n",
    "\n",
    "            if soup.find('div', id = 'map') is not None:\n",
    "                    #latitude\n",
    "                    post_latitude =  soup.find('div', id = 'map')['data-latitude']\n",
    "                    collection_latitude.append(post_latitude)\n",
    "                    #longitude\n",
    "                    post_longitude =  soup.find('div', id = 'map')['data-longitude']\n",
    "                    collection_longitude.append(post_longitude)\n",
    "                    #accuracy\n",
    "                    post_accuracy =  soup.find('div', id = 'map')['data-accuracy']\n",
    "                    collection_accuracy.append(post_accuracy)\n",
    "            else:\n",
    "                    collection_latitude.append(np.nan)\n",
    "                    collection_longitude.append(np.nan)\n",
    "                    collection_accuracy.append(np.nan)\n",
    "                 \n",
    "            # other attributes\n",
    "            attr_groups = soup.find_all('p', class_ = 'attrgroup')\n",
    "            post_attributes = []\n",
    "            for attr_group in attr_groups:\n",
    "                    #print(attr_group.text.strip())\n",
    "                    post_attributes.append(attr_group.text.strip().replace(\"\\n\", \"|\"))\n",
    "            collection_attribute.append(post_attributes) \n",
    "                    \n",
    "            #removes the \\n whitespace from each side, removes the currency symbol, and turns it into an int\n",
    "            #print(post_link, re.findall(r\"\\$\\d+(?:\\.\\d+)?\", post.text)[0].replace(\"$\", \"\"))\n",
    "            post_price = int(re.findall(r\"\\$\\d+(?:\\.\\d+)?\", post.text)[0].replace(\"$\", \"\")) \n",
    "            post_prices.append(post_price)\n",
    "            \n",
    "            if post.find('span', class_ = 'housing') is not None:\n",
    "                \n",
    "                #if the first element is accidentally square footage\n",
    "                if 'ft2' in post.find('span', class_ = 'housing').text.split()[0]:\n",
    "                    \n",
    "                    #make bedroom nan\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #make sqft the first element\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[0][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if the length of the housing details element is more than 2\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) > 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[2][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if there is num bedrooms but no sqft\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) == 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)                    \n",
    "                \n",
    "                else:\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)\n",
    "                \n",
    "            #if none of those conditions catch, make bedroom nan, this won't be needed    \n",
    "            else:\n",
    "                bedroom_count = np.nan\n",
    "                bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                sqft = np.nan\n",
    "                sqfts.append(sqft)\n",
    "                \n",
    "    iterations += 1\n",
    "    print(\"Page \" + str(iterations) + \" scraped successfully.\\n\")\n",
    "    \n",
    "print(\"Scrape complete! Number of records:\", len(post_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "scraped_data = pd.DataFrame({'posted': post_timing,\n",
    "                       'neighborhood': post_hoods,\n",
    "                       'post title': post_title_texts,\n",
    "                       'number bedrooms (from title)': bedroom_counts,\n",
    "                        'sqft': sqfts,\n",
    "                        'URL': post_links,\n",
    "                       'price': post_prices,\n",
    "                       'body': post_body,\n",
    "                       'latitude': collection_latitude,\n",
    "                       'longitude': collection_longitude,\n",
    "                       'accuracy': collection_accuracy,\n",
    "                       'attributes': collection_attribute})\n",
    "\n",
    "#drop duplicate URLs\n",
    "scraped_data = scraped_data.drop_duplicates(subset='URL')\n",
    "\n",
    "#make the number bedrooms to a float\n",
    "scraped_data['number bedrooms'] = scraped_data['number bedrooms (from title)'].apply(lambda x: float(x))\n",
    "\n",
    "#convert datetime string into datetime object\n",
    "scraped_data['posted'] = pd.to_datetime(scraped_data['posted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##couple visualizations and linear regression for a for a quick analysis\n",
    "\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "\n",
    "pylab.rcParams.update(params)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.scatterplot(x='price', y='sqft', hue='number bedrooms', palette='summer', x_jitter=True, y_jitter=True, s=125, data=scraped_data.dropna())\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel(\"Price\", fontsize=18)\n",
    "plt.ylabel(\"Square Footage\", fontsize=18);\n",
    "plt.title(\"Price vs. Square Footage Colored by Number of Bedrooms\", fontsize=18);\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(x='number bedrooms', y='price', data=scraped_data)\n",
    "plt.xlabel(\"# of bedrooms\");\n",
    "plt.xticks(rotation=75)\n",
    "plt.ylabel(\"Price\");\n",
    "plt.title(\"Prices by # of Bedrooms\");\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.regplot(x='price', y='sqft', data=scraped_data.dropna());\n",
    "plt.title('Price vs. Square Footage Regression Plot');\n",
    "plt.xlabel(\"Price\");\n",
    "plt.ylabel(\"Square Feet\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### features extraction\n",
    "\n",
    "# housing type\n",
    "def tag_housing_type(x):\n",
    "    x = ''.join(x)\n",
    "    if x.find(\"apartment\") != -1:\n",
    "        result = \"apartment\"\n",
    "    elif x.find(\"condo\") != -1:\n",
    "        result = \"condo\"\n",
    "    elif x.find(\"cottage/cabin\") != -1:\n",
    "        result = \"cottage/cabin\"\n",
    "    elif x.find(\"duplex\") != -1:\n",
    "        result = \"duplex\"\n",
    "    elif x.find(\"flat\") != -1:\n",
    "        result = \"flat\"\n",
    "    elif x.find(\"townhouse\") != -1:\n",
    "        result = \"townhouse\"\n",
    "    elif x.find(\"house\") != -1:\n",
    "        result = \"house\"\n",
    "    elif x.find(\"in-law\") != -1:\n",
    "        result = \"in-law\"\n",
    "    elif x.find(\"loft\") != -1:\n",
    "        result = \"loft\"\n",
    "    elif x.find(\"manufactured\") != -1:\n",
    "        result = \"manufactured\"\n",
    "    elif x.find(\"assisted living\") != -1:\n",
    "        result = \"assisted living\"\n",
    "    elif x.find(\"land\") != -1:\n",
    "        result = \"land\"  \n",
    "    else:\n",
    "        result = \"NA\"     \n",
    "    return result\n",
    "\n",
    "# laundry\n",
    "def tag_laundry(x):\n",
    "    x = ''.join(x)\n",
    "    if x.find(\"w/d in unit\") != -1:  \n",
    "        result = \"w/d in unit\"\n",
    "    elif x.find(\"w/d hookups\") != -1:  \n",
    "        result = \"w/d hookups\"\n",
    "    elif x.find(\"laundry in bldg\") != -1:  \n",
    "        result = \"laundry in bldg\"\n",
    "    elif x.find(\"laundry on site\") != -1:  \n",
    "        result = \"laundry on site\"\n",
    "    elif x.find(\"no laundry on site\") != -1:  \n",
    "        result = \"no laundry on site\"\n",
    "    else:\n",
    "        result = \"NA\"     \n",
    "    return result\n",
    "\n",
    "# parking\n",
    "def tag_parking(x):\n",
    "    x = ''.join(x)\n",
    "    if x.find(\"carport\") != -1:  \n",
    "        result = \"carport\"\n",
    "    elif x.find(\"attached garage\") != -1:  \n",
    "        result = \"attached garage\"\n",
    "    elif x.find(\"detached garage\") != -1:  \n",
    "        result = \"detached garage\"\n",
    "    elif x.find(\"off-street parking\") != -1:  \n",
    "        result = \"off-street parking\"\n",
    "    elif x.find(\"street parking\") != -1:  \n",
    "        result = \"street parking\"\n",
    "    elif x.find(\"valet parking\") != -1:  \n",
    "        result = \"valet parking\"\n",
    "    elif x.find(\"no parking\") != -1:  \n",
    "        result = \"no parking\"\n",
    "    else:\n",
    "        result = \"NA\"     \n",
    "    return result\n",
    "# cats ok\n",
    "def tag_cats_ok(x):\n",
    "    x = ''.join(x)\n",
    "    if x.find(\"cats are OK\") != -1:  \n",
    "        result = \"cats ok\"\n",
    "    else:\n",
    "        result = \"NA\"     \n",
    "    return result\n",
    "# dogs ok\n",
    "def tag_dogs_ok(x):\n",
    "    x = ''.join(x)\n",
    "    if x.find(\"dogs are OK\") != -1:  \n",
    "        result = \"dogs ok\"\n",
    "    else:\n",
    "        result = \"NA\"     \n",
    "    return result\n",
    "# furnished\n",
    "def tag_furnished(x):\n",
    "    x = ''.join(x)\n",
    "    if x.find(\"furnished\") != -1:  \n",
    "        result = \"furnished\"\n",
    "    else:\n",
    "        result = \"NA\"     \n",
    "    return result\n",
    "# bedrooms\n",
    "def tag_bedrooms(x):\n",
    "    x = ''.join(x)\n",
    "    if x.find(\"BR\") != -1:  \n",
    "        result = x[:x.find(\"BR\")]\n",
    "    else:\n",
    "        result = \"NA\"     \n",
    "    return result\n",
    "# bathrooms\n",
    "def tag_bathrooms(x):\n",
    "    x = ''.join(x)\n",
    "    if x.find(\"Ba\") != -1:  \n",
    "        result = x[x.find(\"/ \")+1:x.find(\"Ba\")]\n",
    "    else:\n",
    "        result = \"NA\"     \n",
    "    return result\n",
    "\n",
    "# process attributes\n",
    "scraped_data['housing_type'] = scraped_data['attributes'].apply(lambda x: tag_housing_type(x))\n",
    "scraped_data['laundry'] = scraped_data['attributes'].apply(lambda x: tag_laundry(x))\n",
    "scraped_data['parking'] = scraped_data['attributes'].apply(lambda x: tag_parking(x))\n",
    "scraped_data['cats ok'] = scraped_data['attributes'].apply(lambda x: tag_cats_ok(x))\n",
    "scraped_data['dogs ok'] = scraped_data['attributes'].apply(lambda x: tag_dogs_ok(x))\n",
    "scraped_data['furnished'] = scraped_data['attributes'].apply(lambda x: tag_furnished(x))\n",
    "scraped_data['bedrooms'] = scraped_data['attributes'].apply(lambda x: tag_bedrooms(x))\n",
    "scraped_data['bathrooms'] = scraped_data['attributes'].apply(lambda x: tag_bathrooms(x))\n",
    "\n",
    "#scraped_data[['attributes', 'housing_type', 'laundry', 'parking', 'cats ok', 'dogs ok', 'furnished','bedrooms', 'bathrooms']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save clean data to XL for further visualization and analysis\n",
    "scraped_data.to_excel('data.xlsx', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
