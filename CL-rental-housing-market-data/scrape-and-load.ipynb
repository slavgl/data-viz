{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on TWD tutorial:\n",
    "https://towardsdatascience.com/web-scraping-craigslist-a-complete-tutorial-c41cea4f4981?gi=78313d2324de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pygsheets\n",
    "\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from warnings import warn\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import re\n",
    "\n",
    "#import matplotlib.pylab as pylab\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#from IPython.core.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 2 queries!\n",
      "Running ETL for...\n",
      " Chilliwack https://abbotsford.craigslist.org/search/apa?search_distance=6&postal=V2P1A5&availabilityMode=0\n",
      "Running ETL for...\n",
      " Maple Ridge https://vancouver.craigslist.org/search/apa?search_distance=7&postal=V2X8S1&availabilityMode=0\n"
     ]
    }
   ],
   "source": [
    "gc = pygsheets.authorize(service_file='client_secret.json')\n",
    "sh = gc.open_by_url('https://docs.google.com/spreadsheets/d/1hpPWoshP5ASZ-qR2FcMhZDlS8JIUXnoLQklxUYhCb3s')\n",
    "\n",
    "queries = sh[0].get_as_df()\n",
    "print(\"Detected\", len(queries), \"queries!\")\n",
    "\n",
    "for index, row in queries.iterrows():  \n",
    "    str_name = row['name']\n",
    "    str_query = row['link']\n",
    "    \n",
    "    print(\"Running ETL for...\\n\", str_name, str_query)\n",
    "    \n",
    "    #get the first page\n",
    "    response = get(str_query)\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    #get the macro-container for the housing posts\n",
    "    posts = html_soup.find_all('li', class_= 'result-row')\n",
    "\n",
    "    #find the total number of posts to find the limit of the pagination\n",
    "    results_num = html_soup.find('div', class_= 'search-legend')\n",
    "    results_total = int(results_num.find('span', class_='totalcount').text) #pulled the total count of posts as the upper bound of the pages array\n",
    "\n",
    "    print(html_soup.title.string)\n",
    "    print(results_total, \"Records on\", math.ceil(results_total/120), \"page(s)\")\n",
    "\n",
    "    #build out the loop\n",
    "\n",
    "    #each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. So we need to step in size 120 in the np.arange function\n",
    "    pages = np.arange(0, results_total+1, 120)\n",
    "\n",
    "    iterations = 0\n",
    "\n",
    "    post_timing = []\n",
    "    post_hoods = []\n",
    "    post_title_texts = []\n",
    "    bedroom_counts = []\n",
    "    sqfts = []\n",
    "    post_links = []\n",
    "    post_prices = []\n",
    "\n",
    "    collection_latitude = []\n",
    "    collection_longitude = []\n",
    "    collection_accuracy = []\n",
    "    collection_attribute = []\n",
    "    post_body = []\n",
    "\n",
    "    for page in pages:\n",
    "\n",
    "        #get request\n",
    "        response = get(str_query            \n",
    "                       + \"s=\" #the parameter for defining the page number \n",
    "                       + str(page) #the page number in the pages array from earlier\n",
    "                      )\n",
    "\n",
    "        sleep(randint(1,5))\n",
    "\n",
    "        #throw warning for status codes that are not 200\n",
    "        if response.status_code != 200:\n",
    "            warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "        #define the html text\n",
    "        page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        #define the posts\n",
    "        posts = html_soup.find_all('li', class_= 'result-row')\n",
    "\n",
    "        #extract data item-wise\n",
    "        for post in posts:\n",
    "\n",
    "            if post.find('span', class_ = 'result-hood') is not None:\n",
    "\n",
    "                #posting date\n",
    "                #grab the datetime element 0 for date and 1 for time\n",
    "                post_datetime = post.find('time', class_= 'result-date')['datetime']\n",
    "                post_timing.append(post_datetime)\n",
    "\n",
    "                #neighborhoods\n",
    "                post_hood = post.find('span', class_= 'result-hood').text\n",
    "                post_hoods.append(post_hood)\n",
    "\n",
    "                #title text\n",
    "                post_title = post.find('a', class_='result-title hdrlnk')\n",
    "                post_title_text = post_title.text\n",
    "                post_title_texts.append(post_title_text)\n",
    "\n",
    "                #post link\n",
    "                post_link = post_title['href']\n",
    "                post_links.append(post_link)\n",
    "\n",
    "                #parse post at the above link\n",
    "                post_link_response = get(post_link)\n",
    "                soup = BeautifulSoup(post_link_response.text, 'html.parser')   \n",
    "\n",
    "                if soup.find('section', id = 'postingbody') is not None:\n",
    "                        post_body.append(soup.find('section', id = 'postingbody').text.strip())\n",
    "                else:\n",
    "                        post_body.append(np.nan)\n",
    "\n",
    "                if soup.find('div', id = 'map') is not None:\n",
    "                        #latitude\n",
    "                        post_latitude =  soup.find('div', id = 'map')['data-latitude']\n",
    "                        collection_latitude.append(post_latitude)\n",
    "                        #longitude\n",
    "                        post_longitude =  soup.find('div', id = 'map')['data-longitude']\n",
    "                        collection_longitude.append(post_longitude)\n",
    "                        #accuracy\n",
    "                        post_accuracy =  soup.find('div', id = 'map')['data-accuracy']\n",
    "                        collection_accuracy.append(post_accuracy)\n",
    "                else:\n",
    "                        collection_latitude.append(np.nan)\n",
    "                        collection_longitude.append(np.nan)\n",
    "                        collection_accuracy.append(np.nan)\n",
    "\n",
    "                # other attributes\n",
    "                attr_groups = soup.find_all('p', class_ = 'attrgroup')\n",
    "                post_attributes = []\n",
    "                for attr_group in attr_groups:\n",
    "                        #print(attr_group.text.strip())\n",
    "                        post_attributes.append(attr_group.text.strip().replace(\"\\n\", \"|\"))\n",
    "                collection_attribute.append(post_attributes) \n",
    "\n",
    "                #removes the \\n whitespace from each side, removes the currency symbol, and turns it into an int\n",
    "                #print(post_link, re.findall(r\"\\$\\d+(?:\\.\\d+)?\", post.text)[0].replace(\"$\", \"\"))\n",
    "                post_price = int(re.findall(r\"\\$\\d+(?:\\.\\d+)?\", post.text)[0].replace(\"$\", \"\")) \n",
    "                post_prices.append(post_price)\n",
    "\n",
    "                if post.find('span', class_ = 'housing') is not None:\n",
    "\n",
    "                    #if the first element is accidentally square footage\n",
    "                    if 'ft2' in post.find('span', class_ = 'housing').text.split()[0]:\n",
    "\n",
    "                        #make bedroom nan\n",
    "                        bedroom_count = np.nan\n",
    "                        bedroom_counts.append(bedroom_count)\n",
    "\n",
    "                        #make sqft the first element\n",
    "                        sqft = int(post.find('span', class_ = 'housing').text.split()[0][:-3])\n",
    "                        sqfts.append(sqft)\n",
    "\n",
    "                    #if the length of the housing details element is more than 2\n",
    "                    elif len(post.find('span', class_ = 'housing').text.split()) > 2:\n",
    "\n",
    "                        #therefore element 0 will be bedroom count\n",
    "                        bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                        bedroom_counts.append(bedroom_count)\n",
    "\n",
    "                        #and sqft will be number 3, so set these here and append\n",
    "                        sqft = int(post.find('span', class_ = 'housing').text.split()[2][:-3])\n",
    "                        sqfts.append(sqft)\n",
    "\n",
    "                    #if there is num bedrooms but no sqft\n",
    "                    elif len(post.find('span', class_ = 'housing').text.split()) == 2:\n",
    "\n",
    "                        #therefore element 0 will be bedroom count\n",
    "                        bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                        bedroom_counts.append(bedroom_count)\n",
    "\n",
    "                        #and sqft will be number 3, so set these here and append\n",
    "                        sqft = np.nan\n",
    "                        sqfts.append(sqft)                    \n",
    "\n",
    "                    else:\n",
    "                        bedroom_count = np.nan\n",
    "                        bedroom_counts.append(bedroom_count)\n",
    "\n",
    "                        sqft = np.nan\n",
    "                        sqfts.append(sqft)\n",
    "\n",
    "                #if none of those conditions catch, make bedroom nan, this won't be needed    \n",
    "                else:\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "\n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)\n",
    "\n",
    "        iterations += 1\n",
    "        print(\"Page \" + str(iterations) + \" scraped successfully.\\n\")\n",
    "\n",
    "    print(\"Scrape complete! Records extracted:\", len(post_links))\n",
    "\n",
    "    # convert to dataframe\n",
    "    scraped_data = pd.DataFrame({'posted': post_timing,\n",
    "                           'neighborhood': post_hoods,\n",
    "                           'post title': post_title_texts,\n",
    "                           'number bedrooms (from title)': bedroom_counts,\n",
    "                            'sqft': sqfts,\n",
    "                            'URL': post_links,\n",
    "                           'price': post_prices,\n",
    "                           'body': post_body,\n",
    "                           'latitude': collection_latitude,\n",
    "                           'longitude': collection_longitude,\n",
    "                           'accuracy': collection_accuracy,\n",
    "                           'attributes': collection_attribute})\n",
    "\n",
    "    #drop duplicate URLs\n",
    "    scraped_data = scraped_data.drop_duplicates(subset='URL')\n",
    "\n",
    "    #make the number bedrooms to a float\n",
    "    scraped_data['number bedrooms'] = scraped_data['number bedrooms (from title)'].apply(lambda x: float(x))\n",
    "\n",
    "    #convert datetime string into datetime object\n",
    "    scraped_data['posted'] = pd.to_datetime(scraped_data['posted'])\n",
    "\n",
    "    #add identifier: query name and link\n",
    "    scraped_data['query_link'] = str_query\n",
    "    scraped_data['query_name'] = str_name\n",
    "\n",
    "    #add current timestamp\n",
    "    scraped_data['as_of'] = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    ### feature extraction\n",
    "\n",
    "    # housing type\n",
    "    def tag_housing_type(x):\n",
    "        x = ''.join(x)\n",
    "        if x.find(\"apartment\") != -1:\n",
    "            result = \"apartment\"\n",
    "        elif x.find(\"condo\") != -1:\n",
    "            result = \"condo\"\n",
    "        elif x.find(\"cottage/cabin\") != -1:\n",
    "            result = \"cottage/cabin\"\n",
    "        elif x.find(\"duplex\") != -1:\n",
    "            result = \"duplex\"\n",
    "        elif x.find(\"flat\") != -1:\n",
    "            result = \"flat\"\n",
    "        elif x.find(\"townhouse\") != -1:\n",
    "            result = \"townhouse\"\n",
    "        elif x.find(\"house\") != -1:\n",
    "            result = \"house\"\n",
    "        elif x.find(\"in-law\") != -1:\n",
    "            result = \"in-law\"\n",
    "        elif x.find(\"loft\") != -1:\n",
    "            result = \"loft\"\n",
    "        elif x.find(\"manufactured\") != -1:\n",
    "            result = \"manufactured\"\n",
    "        elif x.find(\"assisted living\") != -1:\n",
    "            result = \"assisted living\"\n",
    "        elif x.find(\"land\") != -1:\n",
    "            result = \"land\"  \n",
    "        else:\n",
    "            result = \"NA\"     \n",
    "        return result\n",
    "\n",
    "    # laundry\n",
    "    def tag_laundry(x):\n",
    "        x = ''.join(x)\n",
    "        if x.find(\"w/d in unit\") != -1:  \n",
    "            result = \"w/d in unit\"\n",
    "        elif x.find(\"w/d hookups\") != -1:  \n",
    "            result = \"w/d hookups\"\n",
    "        elif x.find(\"laundry in bldg\") != -1:  \n",
    "            result = \"laundry in bldg\"\n",
    "        elif x.find(\"laundry on site\") != -1:  \n",
    "            result = \"laundry on site\"\n",
    "        elif x.find(\"no laundry on site\") != -1:  \n",
    "            result = \"no laundry on site\"\n",
    "        else:\n",
    "            result = \"NA\"     \n",
    "        return result\n",
    "\n",
    "    # parking\n",
    "    def tag_parking(x):\n",
    "        x = ''.join(x)\n",
    "        if x.find(\"carport\") != -1:  \n",
    "            result = \"carport\"\n",
    "        elif x.find(\"attached garage\") != -1:  \n",
    "            result = \"attached garage\"\n",
    "        elif x.find(\"detached garage\") != -1:  \n",
    "            result = \"detached garage\"\n",
    "        elif x.find(\"off-street parking\") != -1:  \n",
    "            result = \"off-street parking\"\n",
    "        elif x.find(\"street parking\") != -1:  \n",
    "            result = \"street parking\"\n",
    "        elif x.find(\"valet parking\") != -1:  \n",
    "            result = \"valet parking\"\n",
    "        elif x.find(\"no parking\") != -1:  \n",
    "            result = \"no parking\"\n",
    "        else:\n",
    "            result = \"NA\"     \n",
    "        return result\n",
    "    # cats ok\n",
    "    def tag_cats_ok(x):\n",
    "        x = ''.join(x)\n",
    "        if x.find(\"cats are OK\") != -1:  \n",
    "            result = \"cats ok\"\n",
    "        else:\n",
    "            result = \"NA\"     \n",
    "        return result\n",
    "    # dogs ok\n",
    "    def tag_dogs_ok(x):\n",
    "        x = ''.join(x)\n",
    "        if x.find(\"dogs are OK\") != -1:  \n",
    "            result = \"dogs ok\"\n",
    "        else:\n",
    "            result = \"NA\"     \n",
    "        return result\n",
    "    # furnished\n",
    "    def tag_furnished(x):\n",
    "        x = ''.join(x)\n",
    "        if x.find(\"furnished\") != -1:  \n",
    "            result = \"furnished\"\n",
    "        else:\n",
    "            result = \"NA\"     \n",
    "        return result\n",
    "    # bedrooms\n",
    "    def tag_bedrooms(x):\n",
    "        x = ''.join(x)\n",
    "        if x.find(\"BR\") != -1:  \n",
    "            result = x[:x.find(\"BR\")]\n",
    "        else:\n",
    "            result = \"NA\"     \n",
    "        return result\n",
    "    # bathrooms\n",
    "    def tag_bathrooms(x):\n",
    "        x = ''.join(x)\n",
    "        if x.find(\"Ba\") != -1:  \n",
    "            result = x[x.find(\"/ \")+1:x.find(\"Ba\")]\n",
    "        else:\n",
    "            result = \"NA\"     \n",
    "        return result\n",
    "\n",
    "    # process attributes\n",
    "    scraped_data['housing_type'] = scraped_data['attributes'].apply(lambda x: tag_housing_type(x))\n",
    "    scraped_data['laundry'] = scraped_data['attributes'].apply(lambda x: tag_laundry(x))\n",
    "    scraped_data['parking'] = scraped_data['attributes'].apply(lambda x: tag_parking(x))\n",
    "    scraped_data['cats ok'] = scraped_data['attributes'].apply(lambda x: tag_cats_ok(x))\n",
    "    scraped_data['dogs ok'] = scraped_data['attributes'].apply(lambda x: tag_dogs_ok(x))\n",
    "    scraped_data['furnished'] = scraped_data['attributes'].apply(lambda x: tag_furnished(x))\n",
    "    scraped_data['bedrooms'] = scraped_data['attributes'].apply(lambda x: tag_bedrooms(x))\n",
    "    scraped_data['bathrooms'] = scraped_data['attributes'].apply(lambda x: tag_bathrooms(x))\n",
    "\n",
    "    # load to gSheet\n",
    "    gc = pygsheets.authorize(service_file='client_secret.json')\n",
    "    sh = gc.open_by_url('https://docs.google.com/spreadsheets/d/1bhQFfEJHaWTfKd9o6cfq0AQ3FO0a-YnvG2DfV7NNAnw')\n",
    "\n",
    "    existing = sh[0].get_as_df()\n",
    "    updated = existing.append(scraped_data)\n",
    "    sh[0].set_dataframe(updated, (1,1))\n",
    "\n",
    "    print(str_query, \"\\nData successfully loaded to gSheet!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
